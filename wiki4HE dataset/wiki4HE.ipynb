{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wiki4HE DATASET\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Information:-\n",
    "\n",
    "Ongoing research on university faculty perceptions and practices of using Wikipedia as a teaching resource. Based on a Technology Acceptance Model, the relationships within the internal and external constructs of the model are analyzed. Both the perception of colleaguesâ€™ opinion about Wikipedia and the perceived quality of the information in Wikipedia play a central role in the obtained model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribute Information:-\n",
    "\n",
    "AGE: numeric\n",
    "\n",
    "GENDER: 0=Male; 1=Female\n",
    "\n",
    "DOMAIN: 1=Arts & Humanities; 2=Sciences; 3=Health Sciences; 4=Engineering & Architecture; 5=Law & Politics\n",
    "\n",
    "PhD: 0=No; 1=Yes\n",
    "\n",
    "YEARSEXP (years of university teaching experience): numeric\n",
    "\n",
    "UNIVERSITY: 1=UOC; 2=UPF\n",
    "\n",
    "UOC_POSITION (academic position of UOC members): 1=Professor; 2=Associate; 3=Assistant; 4=Lecturer; 5=Instructor; 6=Adjunct\n",
    "\n",
    "OTHER (main job in another university for part-time members): 1=Yes; 2=No\n",
    "\n",
    "OTHER_POSITION (work as part-time in another university and UPF members): 1=Professor; 2=Associate; 3=Assistant; 4=Lecturer; 5=Instructor; 6=Adjunct\n",
    "\n",
    "USERWIKI (Wikipedia registered user): 0=No; 1=Yes\n",
    "\n",
    "The following survey items are Likert scale (1-5) ranging from strongly disagree / never (1) to strongly agree / always (5)\n",
    "\n",
    "Perceived Usefulness:-\n",
    "\n",
    "PU1: The use of Wikipedia makes it easier for students to develop new skills\n",
    "\n",
    "PU2: The use of Wikipedia improves students' learning\n",
    "\n",
    "PU3: Wikipedia is useful for teaching\n",
    "\n",
    "Perceived Ease of Use:-\n",
    "\n",
    "PEU1: Wikipedia is user-friendly\n",
    "\n",
    "PEU2: It is easy to find in Wikipedia the information you seek\n",
    "\n",
    "PEU3: It is easy to add or edit information in Wikipedia\n",
    "\n",
    "Perceived Enjoyment:-\n",
    "\n",
    "ENJ1: The use of Wikipedia stimulates curiosity\n",
    "\n",
    "ENJ2: The use of Wikipedia is entertaining\n",
    "\n",
    "Quality:-\n",
    "QU1: Articles in Wikipedia are reliable\n",
    "\n",
    "QU2: Articles in Wikipedia are updated\n",
    "\n",
    "QU3: Articles in Wikipedia are comprehensive\n",
    "\n",
    "QU4: In my area of expertise, Wikipedia has a lower quality than other educational resources\n",
    "\n",
    "QU5: I trust in the editing system of Wikipedia\n",
    "\n",
    "Visibility:-\n",
    "\n",
    "VIS1: Wikipedia improves visibility of students' work\n",
    "\n",
    "VIS2: It is easy to have a record of the contributions made in Wikipedia\n",
    "\n",
    "VIS3: I cite Wikipedia in my academic papers\n",
    "\n",
    "Social Image:-\n",
    "\n",
    "IM1: The use of Wikipedia is well considered among colleagues\n",
    "\n",
    "IM2: In academia, sharing open educational resources is appreciated\n",
    "\n",
    "IM3: My colleagues use Wikipedia\n",
    "\n",
    "Sharing attitude:-\n",
    "\n",
    "SA1: It is important to share academic content in open platforms\n",
    "\n",
    "SA2: It is important to publish research results in other media than academic journals or books\n",
    "\n",
    "SA3: It is important that students become familiar with online collaborative environments\n",
    "\n",
    "Use behaviour:-\n",
    "\n",
    "USE1: I use Wikipedia to develop my teaching materials\n",
    "\n",
    "USE2: I use Wikipedia as a platform to develop educational activities with students\n",
    "\n",
    "USE3: I recommend my students to use Wikipedia\n",
    "\n",
    "USE4: I recommend my colleagues to use Wikipedia\n",
    "\n",
    "USE5: I agree my students use Wikipedia in my courses\n",
    "\n",
    "Profile 2.0:-\n",
    "\n",
    "PF1: I contribute to blogs\n",
    "\n",
    "PF2: I actively participate in social networks\n",
    "\n",
    "PF3: I publish academic content in open platforms\n",
    "\n",
    "Job relevance:-\n",
    "\n",
    "JR1: My university promotes the use of open collaborative environments in the Internet\n",
    "\n",
    "JR2: My university considers the use of open collaborative environments in the Internet as a teaching merit\n",
    "\n",
    "Behavioral intention:-\n",
    "\n",
    "BI1: In the future I will recommend the use of Wikipedia to my colleagues and students\n",
    "\n",
    "BI2: In the future I will use Wikipedia in my teaching activity\n",
    "\n",
    "Incentives:-\n",
    "\n",
    "INC1: To design educational activities using Wikipedia, it would be helpful: a best practices guide\n",
    "\n",
    "INC2: To design educational activities using Wikipedia, it would be helpful: getting instruction from a colleague\n",
    "\n",
    "INC3: To design educational activities using Wikipedia, it would be helpful: getting specific training\n",
    "\n",
    "INC4: To design educational activities using Wikipedia, it would be helpfull: greater institutional recognition\n",
    "\n",
    "Experience:-\n",
    "\n",
    "EXP1: I consult Wikipedia for issues related to my field of expertise\n",
    "\n",
    "EXP2: I consult Wikipedia for other academic related issues\n",
    "\n",
    "EXP3: I consult Wikipedia for personal issues\n",
    "\n",
    "EXP4: I contribute to Wikipedia (editions, revisions, articles improvement...)\n",
    "\n",
    "EXP5: I use wikis to work with my students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will be implementing the following steps to achieve the following result:-\n",
    "\n",
    "1. Importing the necessary libraries.\n",
    "2. Importing the dataset.\n",
    "3. Exploratory data analysis.\n",
    "4. Performing feature engineering i.e. modifying the existing variables and creating new ones for analysis.\n",
    "5. Building the model.\n",
    "6. Visualising the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-1: Importing the necessary libraries:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np for processing data\n",
    "\n",
    "\n",
    "# import pandas as pd for importing the data and working with data\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt for visualisation\n",
    "\n",
    "\n",
    "# import seaborn as sns for data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-2: Importing the dataset:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dataset using pd.read_csv(filename) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeing the dataset:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using dataset.head() we can see the datsaset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-3: Exploratory Data Analysis:-\n",
    "\n",
    "It is common for data scientists to spend a majority of their time exploring and cleaning data, but approaching this as an opportunity to invest in your model (instead of viewing it as just another chore on your to-do list) will yield big dividends later on in the data science process.\n",
    "\n",
    "Performing thorough exploratory data analysis (EDA) and cleaning the dataset are not only essential steps, but also a great opportunity to lay the foundation for a strong machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeing the shape and size of the dataset:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using dataset.shape and dataset.size see the shape and the size of dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeing the non null values in the dataset:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using dataset.info() we can see the non null values in the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describing the dataset:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using dataset.describe() describe the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeing the null values in the dataset:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using dataset.isnull().sum() we can see the null values in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see some nan values in our dataset we can replace them by using dataset.replace()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeing the list of all the columns in the dataset:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using dataset.columns we can see the list of all the columns in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the data using seaborn library\n",
    "\n",
    "There are different types of plots like bar plot, box plot, scatter plot etc.\n",
    "\n",
    "Scatter plot is very useful when we are analyzing the relation ship between 2 features on x and y axis.\n",
    "\n",
    "In seaborn library we have pairplot function which is very useful to scatter plot all the features at once instead of plotting them individually.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the pairplots:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sns.pairplot(data) we can visualize the relationship between each feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the heatmap:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sns.heatmap(dataset.corr(),annot=True) we can visualise the heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-4: Performing Feature Engineering\n",
    "\n",
    "We will be performing the following 3 steps:\n",
    "\n",
    "1.Standard Scaler\n",
    "\n",
    "2.Normalization\n",
    "\n",
    "3.Principal Component Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Scaler:\n",
    "\n",
    "Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance).\n",
    "\n",
    "For instance many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the L1 and L2 regularizers of linear models) assume that all features are centered around 0 and have variance in the same order. If a feature has a variance that is orders of magnitude larger that others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected\n",
    "\n",
    "Standardize features by removing the mean and scaling to unit variance.\n",
    "\n",
    "The standard score of a sample x is calculated as:\n",
    "\n",
    "z = (x - u) / s\n",
    "\n",
    "where u is the mean of the training samples, and s is the standard deviation of the training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to implement the standard scaler first create an object for StandardScaler\n",
    "\n",
    "\n",
    "# now perform scaler.fit() to fit the data\n",
    "\n",
    "\n",
    "# now perform scaler.transform() to get the scaled data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization:\n",
    "\n",
    "Normalization is used to scale the data of an attribute so that it falls in a smaller range, such as -1.0 to 1.0 or 0.0 to 1.0. It is generally useful for classification algorithms.\n",
    "\n",
    "Normalization is generally required when we are dealing with attributes on a different scale, otherwise, it may lead to a dilution in effectiveness of an important equally important attribute(on lower scale) because of other attribute having values on larger scale.\n",
    "In simple words, when multiple attributes are there but attributes have values on different scales, this may lead to poor data models while performing data mining operations. So they are normalized to bring all the attributes on the same scale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now implement the normalization to normalize the data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "Principal Components Analysis is an unsupervised learning class of statistical techniques used to explain data in high dimension using smaller number of variables called the principal components.\n",
    "\n",
    "Assuming we have a set X made up of n measurements each represented by a set of p features, X1, X2, … , Xp. If we want to plot this data in a 2-dimensional plane, we can plot n measurements using two features at a time. If the number of features are more than 3 or four then plotting this in two dimension will be a challenge as the number of plots would be p(p-1)/2 which would be hard to plot.\n",
    "We would like to visualize this data in two dimension without losing information contained in the data. This is what PCA allows us to do.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so to implement PCA first we need to create an object for PCA and also need to mention that how many dimensions we need finally\n",
    "\n",
    "\n",
    "# now do pca.fit(data) to fit the data\n",
    "\n",
    "\n",
    "# now do pca.transform(data) to transform the higher-dimensionality data to lower dimensions\n",
    "\n",
    "\n",
    "# now after implementing pca, use pd.DataFrame(data) to convert the new data into a Data Frame else\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-5: Building The model\n",
    "\n",
    "The algorithm works as follows:\n",
    "\n",
    "1. First we initialize k points, called means, randomly.\n",
    "\n",
    "2. We categorize each item to its closest mean and we update the mean’s coordinates, which are the averages of the items categorized in that mean so far.\n",
    "\n",
    "3. We repeat the process for a given number of iterations and at the end, we have our clusters.\n",
    "\n",
    "So, basically we will be following two steps:-\n",
    "\n",
    "1. Implementing the Elbow method which we will return the optimal value of clusters to be formed.\n",
    "\n",
    "2. We will implement K-Means algorithm to create the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Elbow Method\n",
    "\n",
    "In cluster analysis, the elbow method is a heuristic used in determining the number of clusters in a data set. The method consists of plotting the explained variation as a function of the number of clusters, and picking the elbow of the curve as the number of clusters to use. The same method can be used to choose the number of parameters in other data-driven models, such as the number of principal components to describe a data set.\n",
    "\n",
    "A fundamental step for any unsupervised algorithm is to determine the optimal number of clusters into which the data may be clustered. The Elbow Method is one of the most popular methods to determine this optimal value of k.\n",
    "\n",
    "To determine the optimal number of clusters, we have to select the value of k at the “elbow” ie the point after which the distortion/inertia start decreasing in a linear fashion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to implement the elbow method first create an empty list name it as wcss\n",
    "\n",
    "\n",
    "# now initiate a for loop ranging between (1,11) and implement k-means clustering for every i number of clusters\n",
    "\n",
    "\n",
    "# keep appending the empty list with the kmeans.inertia_ values \n",
    "\n",
    "\n",
    "# now plot the graph between the range(1,11) and the wcss \n",
    "\n",
    "\n",
    "# from the plot we determine the optimal value of k\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing K-Means:\n",
    "\n",
    "K-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. It is popular for cluster analysis in data mining. k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. For instance, Better Euclidean solutions can be found using k-medians and k-medoids.\n",
    "\n",
    "The above algorithm in pseudocode: \n",
    "\n",
    "Initialize k means with random values\n",
    "\n",
    "For a given number of iterations:\n",
    "    \n",
    "Iterate through items\n",
    "\n",
    "Use kmeans with different number of clusters\n",
    "\n",
    "Append the list with kmeans.inertia_ values\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now with the optimal k value implement the K-Means Clustering Algorithm\n",
    "\n",
    "\n",
    "# now using kmeans.fit_predict to predict that which data belong to which cluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-: Visualising the results:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to visualise the final result use plt.scatter() with respective arguments to view the created clusters\n",
    "\n",
    "\n",
    "# also plot the centroids of the respective clusters using kmeans.cluster_centers_  \n",
    "\n",
    "\n",
    "# finally you will be able to look at the result using plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
