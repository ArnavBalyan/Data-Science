{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "In this project, you will be using a dataset for predicting room occupancy with using environmental observations such as temperature, humidity and CO2 level. This is an experimental dataset, these predictions might help Heating, Ventilating and Air Conditioning (HVAC) sector. For instance, we are using sensors like thermostats to get information about the environment and with that info our system decides to heat or not situation. But if the thermostat set manually by a occupant before and there is no more occupants in the environment, what then? The system won't shutdown until it gets set values, and this situation will lead high energy consumption. \n",
    "\n",
    "\n",
    " We've provided some of the code, but left the implementation of the model building up to you (for the most part). After you've submitted this project, feel free to explore the data and the model more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Importing Libraries\n",
    "\n",
    "In the code below are given some libraries which we are going to use throughout this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "\n",
    "# Importing necessary libraries for this notebook.\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.offline import init_notebook_mode, iplot, plot\n",
    "init_notebook_mode(connected=True)\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.regularizers import l2, l1\n",
    "from keras.metrics import BinaryAccuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Reading the Dataset\n",
    "\n",
    "A critical step in working with machine learning models is preparing the data correctly. Variables on different scales make it difficult for the network to efficiently learn the correct weights. Below, we've written the code to load and prepare the data. You'll learn more about this soon!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mention the datapath\n",
    "\n",
    "data_path_test=''\n",
    "data_path_test2=''\n",
    "data_path_training=''\n",
    "\n",
    "# load the dataset\n",
    "\n",
    "datatest = pd.read_csv(\"data_path_test\")\n",
    "datatest2 = pd.read_csv(\"data_path_test2\")\n",
    "datatraining = pd.read_csv(\"data_path_training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Exploratory Data Analysis\n",
    "\n",
    "* Exploratory Data Analysis refers to the critical process of performing initial investigations on data so as to discover patterns,to spot anomalies,to test hypothesis and to check assumptions with the help of summary statistics and graphical representations.\n",
    "\n",
    "* It is a good practice to understand the data first and try to gather as many insights from it. EDA is all about making sense of data in hand,before getting them dirty with it.\n",
    "\n",
    "* There are 3 types of EDA Analysis, we will do them all step by step.\n",
    "\n",
    "We have three different .txt file as datatest, datatest2 and datatraining. Lets analyze them step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Uni-variate Analysis\n",
    "\n",
    "* “Uni” means one and “Variate” means variable hence univariate analysis means analysis of one variable or one feature. Univariate basically tells us how data in each feature is distributed and also tells us about central tendencies like mean, median, and mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datatest.info())\n",
    "datatest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print datatest2 info and its head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print datatraining info and its head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the univariate analysis of data you will notice that All text files has seven columns as date, temperature, humidity, light, CO2, humidity ratio and occupancy. \n",
    "* Temperature in Celsius.\n",
    "* Relative humidity as a percentage.\n",
    "* Light measured in lux.\n",
    "* Carbon dioxide measured in parts per million.\n",
    "* Humidity ratio, derived from temperature and relative humidity measured in kilograms of water vapor per kilogram of air.\n",
    "* Occupancy as either 1 for occupied or 0 for not occupied.\n",
    "\n",
    "In this project For training and testing the models, you will use datatraining(8143 instances) as training, datatest(2665 instances) as validation and datatest2(9752 instances) as test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting date attribute to pandas datetime format\n",
    "\n",
    "## start code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## end code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describing the dataset\n",
    "\n",
    "datatraining.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint**\n",
    "\n",
    "In the above part you had noticed that values are low for humidity_ratio and high for light and CO2, you should normalize the data to simplfy the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the dataset\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "columns = ['Temperature', 'Humidity', 'Light', 'CO2', 'HumidityRatio']\n",
    "\n",
    "## start code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## end code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets analyze all numerical features of this dataset (except our target variable) using Box Plots.\n",
    "\n",
    "**What are Box Plots??**\n",
    "\n",
    "* Seaborn boxplot is a very basic plot Boxplots are used to visualize distributions. Thats very useful when you want to compare data between two groups. Sometimes a boxplot is named a box-and-whisker plot. Any box shows the quartiles of the dataset while the whiskers extend to show the rest of the distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a boxplot\n",
    "\n",
    "## start code \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## end code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Multi-Variate Analysis\n",
    "\n",
    "* Multivariate analysis (MVA) is based on the principles of multivariate statistics, which involves observation and analysis of more than one statistical outcome variable at a time. Typically, MVA is used to address the situations where multiple measurements are made on each experimental unit and the relations among these measurements and their structures are important.\n",
    "* Essentially, multivariate analysis is a tool to find patterns and relationships between several variables simultaneously. It lets us predict the effect a change in one variable will have on other variables. ... This gives multivariate analysis a decisive advantage over other forms of analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets analyze some closely corelating features of this dataset by making a heatmap using seaborn library.\n",
    "\n",
    "**What is a heatmap??**\n",
    "* A heatmap is a two-dimensional graphical representation of data where the individual values that are contained in a matrix are represented as colors. The seaborn python package allows the creation of annotated heatmaps which can be tweaked using Matplotlib tools as per the creator's requirement.\n",
    "* Image below is of a heatmap.\n",
    "\n",
    "<img src=\"https://d1rwhvwstyk9gu.cloudfront.net/2017/07/seaburn-2.png\"\n",
    "     width=\"800px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a heatmap\n",
    "\n",
    "## start code\n",
    "\n",
    "\n",
    "\n",
    "## end code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets analyze some correlating features of this dataset using a 3 dimension scatter plot.\n",
    "You will use \"Temperature\", \"Humidity\", \"Co2\" as your 3 dimensions and you will also \"Light\" as size of these dots as our 4th dimension ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datatraining.copy()\n",
    "data.Occupancy = data.Occupancy.astype(str)\n",
    "fig = px.scatter_3d(data, x='Temperature', y='Humidity', z='CO2', size='Light', color='Occupancy', color_discrete_map={'1':'red', '0':'blue'})\n",
    "fig.update_layout(scene_zaxis_type=\"log\", title={'text': \"Features and Occupancy\",\n",
    "                                                'y':0.9,\n",
    "                                                'x':0.5,\n",
    "                                                'xanchor': 'center',\n",
    "                                                'yanchor': 'top'})\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look on the 4-dimensional plot for occupancy. The 4th dimension is size of dots here and you used light value as 4th dimension. The higher light will lead to bigger dots and the lower light will lead to smaller dots. You can use your mouse to change your perspective and take a closer look on the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check distribution of target column\n",
    "\n",
    "## start code \n",
    "\n",
    "\n",
    "\n",
    "## end code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint**\n",
    "\n",
    "Don't you think that the data is unbalanced, so you need to find another relations between features to strengthen our predictions. I have a question at this point, is there any relation between occupancy and the hour of the day? Let's look into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours_1 = []\n",
    "hours_0 = []\n",
    "for date in datatraining[datatraining['Occupancy'] == 1]['date']:\n",
    "    hours_1.append(date.hour)\n",
    "for date in datatraining[datatraining['Occupancy'] == 0]['date']:\n",
    "    hours_0.append(date.hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "ax = sns.distplot(hours_1)\n",
    "ax = sns.distplot(hours_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above histogram, what can you say? Between 07:00 and 18:00 there are occupants in the environment or not. But the time come to non-working hours, then you can absolutely say that there is no occupant. With this information, you will create a new feature from date column as day period.\n",
    "* 07:00 - 18:00 working hour (labeled as 1)\n",
    "* rest of the day non-working hour (labeled as 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new feature \"period_of_day\" from date column\n",
    "\n",
    "## start code\n",
    "\n",
    "\n",
    "\n",
    "## end code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separte the target variables from the all the datasets\n",
    "\n",
    "## start code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## end code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. KNN (K-Nearest Neighbors)\n",
    "Let's try different hyperparameters on KNN model such as n_neighbors, weights and metrics to find best options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter-tuning for knn\n",
    "# compllete the below code\n",
    "\n",
    "n_neighbors_list = [7,15,45,135]\n",
    "weights_list = ['uniform', 'distance']\n",
    "metric_list = ['euclidean', 'manhattan']\n",
    "accuracies = {}\n",
    "\n",
    "\n",
    "## start code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## end code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdata = pd.DataFrame()\n",
    "plotdata['Parameters'] = accuracies.keys()\n",
    "plotdata['Accuracy'] = accuracies.values()\n",
    "fig = px.line(plotdata, x=\"Parameters\", y=\"Accuracy\")\n",
    "fig.update_layout(title={'text': \"Accuracies for Different Hyper-Parameters\",\n",
    "                                                'x':0.5,\n",
    "                                                'xanchor': 'center',\n",
    "                                                'yanchor': 'top'})\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking over the accuracies graph:\n",
    "* 135 is enough for k-value.\n",
    "* Manhattan distance performs better when k has low value. If k value is higher than usual euclidean is the better option.\n",
    "* Uniform weights are better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use optimal parameters for model building and printing confusion matrix\n",
    "\n",
    "## start code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## end code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. SVM (Support-Vector Machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model using SVM and print its accuracy on validation dataset\n",
    "\n",
    "## start code\n",
    "\n",
    "\n",
    "\n",
    "## end code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the confusion matrix on validation dataset of SVM model\n",
    "\n",
    "## start code\n",
    "\n",
    "\n",
    "\n",
    "## end code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Machine Learning models doing well with validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Classification with Neural Networks\n",
    "Firsty, you would try different models like with or without regularization methods. You will create four different models:\n",
    "1. Without regularization\n",
    "2. With 0.2 dropout regularization\n",
    "3. With L1(Lasso) regularization\n",
    "4. With L2(Ridge) regularization\n",
    "\n",
    "After all models trained and evaluated with validation data, we will compare the training and validation losses.\n",
    "So out of all 4 one is implemented for you and the rest 3 are left for you to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NN without regularization\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(32, activation='relu', input_dim=6))\n",
    "model1.add(Dense(16, activation='relu'))\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "model1.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history1 = model1.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_validation, y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN with 0.2 dropout ratio before the hidden layer.\n",
    "\n",
    "## start code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## end code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN with L1(Lasso) regularization\n",
    "\n",
    "## start code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## end code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN with L2(Ridge) Regularization\n",
    "\n",
    "## start code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## end code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the losses\n",
    "\n",
    "loss1 = history1.history['loss']\n",
    "val_loss1 = history1.history['val_loss']\n",
    "loss2 = history2.history['loss']\n",
    "val_loss2 = history2.history['val_loss']\n",
    "loss3 = history3.history['loss']\n",
    "val_loss3 = history3.history['val_loss']\n",
    "loss4 = history4.history['loss']\n",
    "val_loss4 = history4.history['val_loss']\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(loss1)), y=loss1,\n",
    "                    name='Training Loss without Regularization', line=dict(color='royalblue')))\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(val_loss1)), y=val_loss1,\n",
    "                    name='Validation Loss without Regularization', line = dict(color='firebrick')))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(loss2)), y=loss2,\n",
    "                    name='Training Loss with Dropout', line=dict(color='royalblue', dash='dash')))\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(val_loss2)), y=val_loss2,\n",
    "                    name='Validation Loss with Dropout', line = dict(color='firebrick', dash='dash')))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(loss3)), y=loss3,\n",
    "                    name='Training Loss with L1 Regularization', line=dict(color='royalblue', dash='dot')))\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(val_loss3)), y=val_loss3,\n",
    "                    name='Validation Loss with L1 Regularization', line = dict(color='firebrick', dash='dot')))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(loss4)), y=loss4,\n",
    "                    name='Training Loss with L2 Regularization', line=dict(color='royalblue', dash='longdashdot')))\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(val_loss4)), y=val_loss4,\n",
    "                    name='Validation Loss with L2 Regularization', line = dict(color='firebrick', dash='longdashdot')))\n",
    "\n",
    "\n",
    "fig.update_layout(xaxis_title='Epochs',\n",
    "                  yaxis_title='Loss',\n",
    "                  title={'text': \"Training and Validation Losses for Different Models\",\n",
    "                                                'x':0.5,\n",
    "                                                'xanchor': 'center',\n",
    "                                                'yanchor': 'top'})\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint**\n",
    "\n",
    "* NN without regularization is unstabilized as expected.\n",
    "* Dropout and L2 regularization doing well.\n",
    "* L1 regularization is stable but it has biggest loss value.\n",
    "\n",
    "So your best option will be a dropout layer and L2 regularization on layers. Let's train it.\n",
    "\n",
    "P.S. You can click on the legend to close some of lines. It might be useful when examining the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training our final neural network using dropout and L2 regularization\n",
    "\n",
    "## start code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## end code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Comparing Performances of SVM and Neural Network\n",
    "Let's test our models with the test data. This data has nearly 10000 instances. you will evaluate them with accuracy metric first, after then you will look into confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing accuracy scores on test data using SVM and NN\n",
    "\n",
    "## start code\n",
    "\n",
    "\n",
    "## end code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems very close right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing confusion matrix for SVM over test data\n",
    "\n",
    "## start code\n",
    "\n",
    "\n",
    "\n",
    "## end code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing confusion matrix for NN\n",
    "\n",
    "## start code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## end code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Conclusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your conclusions here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
