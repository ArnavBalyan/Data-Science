{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression is one of the easiest algorithms in machine learning. In this notebook this algorithm wil be explored and implemented using Python from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the name suggests this algorithm is applicable for Regression problems. Linear Regression is a Linear Model. Which means, establishing a linear relationship between the input variables(X) and single output variable(Y). When the input(X) is a single variable this model is called Simple Linear Regression and when there are mutiple input variables(X), it is called Multiple Linear Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We discussed that Linear Regression is a simple model. Simple Linear Regression is the simplest model in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem you have an input variable - X and one output variable - Y. And you have to build linear relationship between these variables. Here the input variable is called Independent Variable and the output variable is called Dependent Variable. You can define this linear relationship as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                    y=a+bx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'a' is called as a scale factor or coefficient, and 'b' is called bias coefficient. The bias coeffient gives an extra degree of freedom to this model. This equation is similar to the line equation y = mx + b with m=b(slope) and b=a(intercept). So in this Simple Linear Regression model you have to draw a line between X and Y which estimates the relationship between X and Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find these coefficients there are different approaches like ordinary least square, gradient descent. In this notebook, you will use ordinary least square method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordinary Least Square:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordinary least squares, or linear least squares, estimates the parameters in a regression model by minimizing the sum of the squared residuals. This method draws a line through the data points that minimizes the sum of the squared differences between the observed values and the corresponding fitted values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are going to use a dataset containing modes of advertising and sales of a product. This data set has many features. But, you will not use all of them. You will just deal with the TV  advertising and its impact on the sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Necessary Libraries\n",
    "%matplotlib inline\n",
    "#import numpy as np\n",
    "#import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "# Reading Data\n",
    "#data = pd.read_csv('Downloads/advertising.csv')\n",
    "#print(data.shape)\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting X and Y\n",
    "#X= data.iloc[:,:1].values\n",
    "#Y= data.iloc[:,3:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To find the values of a and b you will need the mean of X and y. You will find these and the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean X and Y\n",
    "#mean_x = np.mean(X)\n",
    "#mean_y = np.mean(y)\n",
    "\n",
    "# Total number of values\n",
    "#m = len(X)\n",
    "\n",
    "# Using the formula to calculate b1 and b2\n",
    "#numer = 0\n",
    "#denom = 0\n",
    "#for i in range(m):\n",
    "    #numer += (X[i] - mean_x) * (Y[i] - mean_y)\n",
    "    #denom += (X[i] - mean_x) ** 2\n",
    "#b = numer / denom\n",
    "#a = mean_y - (b * mean_x)\n",
    "\n",
    "# Print coefficients\n",
    "#print(b, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thus, you have our coeffcients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                     What is the Linear Model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see this graphically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Values and Regression Line\n",
    "\n",
    "#max_x = np.max(X) + 100\n",
    "#min_x = np.min(X) - 100\n",
    "\n",
    "# Calculating line values x and y\n",
    "#x = np.linspace(min_x, max_x, 1000)\n",
    "#y = a + b * x\n",
    "\n",
    "# Ploting Line\n",
    "#plt.plot(x, y, color='#58b970', label='Regression Line')\n",
    "# Ploting Scatter Points\n",
    "#plt.scatter(X, Y, c='#ef5423', label='Scatter Plot')\n",
    "\n",
    "#plt.xlabel('Head Size in cm3')\n",
    "#plt.ylabel('Brain Weight in grams')\n",
    "#plt.legend()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Root Mean Square Error:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Root Mean Square Error (RMSE) is the standard deviation of the residuals (prediction errors). Residuals are a measure of how far from the regression line data points are; RMSE is a measure of how spread out these residuals are. In other words, it tells how concentrated the data is around the line of best fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Root Mean Squares Error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rmse = 0\n",
    "#for i in range(m):\n",
    "    #y_pred = a + b * X[i]\n",
    "    #rmse += (Y[i] - y_pred) ** 2\n",
    "#rmse = np.sqrt(rmse/m)\n",
    "#print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R squared:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-squared is a statistical measure of how close the data are to the fitted regression line. It is also known as the coefficient of determination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-squared is always between 0 and 100%:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating R Squared:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ss_t = 0\n",
    "#ss_r = 0\n",
    "#for i in range(m):\n",
    "    #y_pred = a + b * X[i]\n",
    "    #ss_t += (Y[i] - mean_y) ** 2\n",
    "    #ss_r += (Y[i] - y_pred) ** 2\n",
    "#r2 = 1 - (ss_r/ss_t)\n",
    "#print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The scikit-learn approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn is simple machine learning library in Python. Building Machine Learning models are very easy using scikit-learn.   Building this Simple Linear Regression Model using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.linear_model import LinearRegression\n",
    "#from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Cannot use Rank 1 matrix in scikit learn\n",
    "#X = X.reshape((m, 1))\n",
    "# Creating Model\n",
    "#reg = LinearRegression()\n",
    "# Fitting training data\n",
    "#reg = reg.fit(X, Y)\n",
    "# Y Prediction\n",
    "#Y_pred = reg.predict(X)\n",
    "\n",
    "# Calculating RMSE and R2 Score\n",
    "#mse = mean_squared_error(Y, Y_pred)\n",
    "#rmse = np.sqrt(mse)\n",
    "#r2_score = reg.score(X, Y)\n",
    "\n",
    "#print(np.sqrt(mse))\n",
    "#print(r2_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this similar to the model you built from scratch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
